1. Qual o objetivo do comando cache em Spark?

    O Comando serve para "cachear" um determinado DataSert do RDD em memória, de modo a facilitar o acesso a informação 
    pela aplicação de forma repetitiva, como o uso de um algorítmo interativo ou resultado de uma query.

2. O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?

    Exitem alguns fatores, se não o principal destes fatores é a execução de operações de I\O em disco pelo MapReduce, 
    onde cada resultado de um jobou execuções intermediárias é realizado estas operações, onde em Spark, o mesmo dinamismo 
    ocorre em memória, permitindo ainteração mais rápida de leitura e escrita das informações.

3. Qual é a função do SparkContext?

    É o Master da aplicação SPARK, configurando serviços internos e estabelecendo uma conexão com um ambiente de execução, 
    sendo usado para criar RDDs, acumuladores e variáveis ​​de broadcast, entre outros.

4. Explique com suas palavras o que é Resilient Distributed Datasets (RDD).

    É o conjunto de informações de um cluster que são abstraídos em um DataSet tolerante a falha no qual o SPARK realiza 
    as operações de processamento de dados.

5. GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?

    No Sparkt o reduceByKey funciona por primeiro a cada partição da informação, combinar a saída com uma chave comum 
    antes de trafegar os dados, executar o solicitado e trazer o resultado da operação, por outro lado, ao executar o 
    groupByKey, primeiro é realizado o trafego de todas as informações a serem agrupadas, e posteriormente unidas por 
    pares de chave-valor, necessitando realizar um trafego maior de informações para poder chegar ao mesmo resultado.

6. Explique o que o código Scala abaixo faz.

    val textFile = sc.textFile("hdfs://...")
    val counts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
    counts.saveAsTextFile("hdfs://...")

    Um arquivo texto (Text File) existente em um HDFS é lido e salvo no SparkContext, depois é realizada uma 
    contagem salva em uma variável (val counts), onde é criado um MAP de chave-valor, em que as chaves (palavra - word)
    são salvas com o valor 1 atribuídos a elas, e feito esta separação, é realizada a soma dos valores por chave, onde 
    a agregação por chave resulta em uma contagem de palavras (palavra, contagem) e este resultado é salvo como arquivo texto
    no mesmo HDFS do arquivo de entrada.